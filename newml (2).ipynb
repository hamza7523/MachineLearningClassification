{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Load datasets\ntrain_data = pd.read_csv('train_set.csv')\ntest_data = pd.read_csv('test_set.csv')\n\n# Separate features and target\nX_train = train_data.drop(['Y', 'RecordId'], axis=1)\ny_train = train_data['Y']\nX_test = test_data.drop('RecordId', axis=1)\n\n# Step 1: Handle missing values using SimpleImputer\nprint(\"Handling missing values...\")\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.transform(X_test)\n\n# Step 2: Apply correlation filter to X_train and match for X_test\nprint(\"Applying correlation filter...\")\nX_train_df = pd.DataFrame(X_train, columns=train_data.drop(['Y', 'RecordId'], axis=1).columns)  # Preserve column names\ncorrelation_matrix = X_train_df.corr().abs()\nupper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\n# Set a threshold and drop highly correlated features from X_train\ncorrelation_threshold = 0.88\nto_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\nX_train_df = X_train_df.drop(columns=to_drop)\n\n# Apply the same feature drops to X_test to match feature count\nX_test_df = pd.DataFrame(X_test, columns=test_data.drop(['RecordId'], axis=1).columns).drop(columns=to_drop, errors='ignore')\n\n# Convert back to numpy arrays\nX_train = X_train_df.values\nX_test = X_test_df.values\n\n# Step 3: Normalize data using MinMaxScaler\nprint(\"Normalizing data...\")\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Step 4: Initialize XGBoost classifier\nxgb = XGBClassifier(\n    n_estimators=11000,\n    learning_rate=0.004,\n    max_depth=3,\n    subsample=0.73,\n    colsample_bytree=0.73,\n    eval_metric='logloss',\n    gamma=0.1,\n    reg_alpha=0.01,\n    reg_lambda=1\n)\n\n# Step 5: Perform Stratified K-Fold Cross-Validation\nprint(\"Performing Stratified K-Fold Cross-Validation...\")\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_accuracies = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n    \n    xgb.fit(X_train_fold, y_train_fold)\n    val_accuracy = xgb.score(X_val_fold, y_val_fold)\n    fold_accuracies.append(val_accuracy)\n    print(f\"Fold {fold + 1} Accuracy: {val_accuracy:.2f}\")\n\navg_accuracy = np.mean(fold_accuracies)\nprint(f\"\\nAverage Stratified K-Fold Accuracy: {avg_accuracy:.2f}\")\n\n# Train the model on the entire training dataset\nxgb.fit(X_train, y_train)\n\n# Step 7: Predict on the test set\nprint(\"Predicting on the test set...\")\ny_test_pred_prob = xgb.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n\n# Step 8: Prepare the submission file\nsubmission = pd.DataFrame({\n    'RecordId': test_data['RecordId'],\n    'Y': y_test_pred_prob\n})\nsubmission.to_csv('XGBoosttest1.csv', index=False)\nprint(\"Submission file 'XGBoosttest1.csv' created successfully.\")\n\n# Step 9: Generate ROC curve for validation set\nprint(\"Generating ROC curve...\")\ny_val_prob = xgb.predict_proba(X_val_fold)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_val_fold, y_val_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, label=f'XGBoost ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for XGBoost Classifier')\nplt.legend(loc='best')\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}